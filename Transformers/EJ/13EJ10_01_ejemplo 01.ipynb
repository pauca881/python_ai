{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, n_heads, num_layers, ff_dim, output_dim):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Embedding para la entrada\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 100, emb_dim))  # 100: Máxima longitud\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=ff_dim)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Capa final para clasificación (o regresión)\n",
    "        self.fc = nn.Linear(emb_dim, output_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Agregar embeddings y codificación posicional\n",
    "        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        \n",
    "        # Pasar por el codificador Transformer\n",
    "        encoded = self.encoder(src)\n",
    "        \n",
    "        # Promediar la salida y pasar por una capa lineal\n",
    "        output = self.fc(encoded.mean(dim=1))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 1000  # Vocabulario\n",
    "emb_dim = 128\n",
    "n_heads = 4\n",
    "num_layers = 3\n",
    "ff_dim = 256\n",
    "output_dim = 10  # Por ejemplo, 10 clases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fundacion\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Crear modelo, pérdida y optimizador\n",
    "model = Transformer(input_dim, emb_dim, n_heads, num_layers, ff_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos simulados\n",
    "x = torch.randint(0, input_dim, (32, 50))  # Batch de 32, secuencia de longitud 50\n",
    "y = torch.randint(0, output_dim, (32,))   # Etiquetas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.348327159881592\n",
      "Epoch 2, Loss: 2.164961338043213\n",
      "Epoch 3, Loss: 2.1168925762176514\n",
      "Epoch 4, Loss: 2.0769429206848145\n",
      "Epoch 5, Loss: 2.0449275970458984\n",
      "Epoch 6, Loss: 2.0086050033569336\n",
      "Epoch 7, Loss: 1.9800664186477661\n",
      "Epoch 8, Loss: 1.9423402547836304\n",
      "Epoch 9, Loss: 1.8996480703353882\n",
      "Epoch 10, Loss: 1.8483021259307861\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento simple\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
