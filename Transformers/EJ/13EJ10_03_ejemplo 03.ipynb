{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (2.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\fundacion\\anaconda3\\envs\\transformers\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos simulados\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab_size=1000, seq_length=10):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        # Relleno o truncado de la secuencia\n",
    "        text = text[:self.seq_length] + [0] * max(0, self.seq_length - len(text))\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos aleatorios\n",
    "def generate_data(num_samples=1000, vocab_size=1000, seq_length=10):\n",
    "    texts = [np.random.randint(1, vocab_size, size=np.random.randint(5, seq_length + 1)).tolist() for _ in range(num_samples)]\n",
    "    labels = np.random.randint(0, 2, size=num_samples).tolist()  # Etiquetas binarias\n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de entrenamiento y validación\n",
    "train_texts, train_labels = generate_data(800)\n",
    "val_texts, val_labels = generate_data(200)\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels)\n",
    "val_dataset = TextDataset(val_texts, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Transformer\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_heads, num_layers, ff_dim, num_classes, max_seq_len):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=ff_dim)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(emb_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        encoded = self.encoder(embedded)\n",
    "        output = self.fc(encoded.mean(dim=1))  # Promedio sobre la secuencia\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fundacion\\anaconda3\\envs\\transformers\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Instanciar el modelo\n",
    "vocab_size = 1000\n",
    "emb_dim = 128\n",
    "n_heads = 4\n",
    "num_layers = 2\n",
    "ff_dim = 256\n",
    "num_classes = 2\n",
    "max_seq_len = 10\n",
    "model = TransformerModel(vocab_size, emb_dim, n_heads, num_layers, ff_dim, num_classes, max_seq_len).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.7222, Val Loss: 0.7006, Val Accuracy: 0.4700\n",
      "Epoch 2/10, Train Loss: 0.6863, Val Loss: 0.6912, Val Accuracy: 0.5400\n",
      "Epoch 3/10, Train Loss: 0.6448, Val Loss: 0.6904, Val Accuracy: 0.5850\n",
      "Epoch 4/10, Train Loss: 0.5797, Val Loss: 0.7794, Val Accuracy: 0.4900\n",
      "Epoch 5/10, Train Loss: 0.4920, Val Loss: 0.9459, Val Accuracy: 0.4750\n",
      "Epoch 6/10, Train Loss: 0.3834, Val Loss: 1.0476, Val Accuracy: 0.5350\n",
      "Epoch 7/10, Train Loss: 0.3156, Val Loss: 1.1766, Val Accuracy: 0.5050\n",
      "Epoch 8/10, Train Loss: 0.2701, Val Loss: 1.4347, Val Accuracy: 0.5200\n",
      "Epoch 9/10, Train Loss: 0.2143, Val Loss: 1.4707, Val Accuracy: 0.5050\n",
      "Epoch 10/10, Train Loss: 0.1910, Val Loss: 1.5714, Val Accuracy: 0.5100\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
