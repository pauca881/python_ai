{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fundacion\\anaconda3\\envs\\transformers\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración inicial\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de poesía simple\n",
    "corpus = [\n",
    "    \"En el cielo las estrellas cantan\",\n",
    "    \"La luna brilla con todo su esplendor\",\n",
    "    \"Un río tranquilo acaricia la montaña\",\n",
    "    \"El amor eterno nunca pierde su valor\",\n",
    "    \"En el bosque los árboles susurran secretos\",\n",
    "    \"Las flores bailan con el viento al pasar\",\n",
    "    \"El sol se esconde dejando el mundo en calma\",\n",
    "    \"Una melodía eterna resuena en el mar\",\n",
    "    \"La brisa del amanecer despierta mi alma\",\n",
    "    \"Los sueños se dibujan al borde del horizonte\",\n",
    "    \"Un susurro de hojas acompaña mi caminar\",\n",
    "    \"El río canta historias de un tiempo lejano\",\n",
    "    \"Las estrellas observan el silencio de la noche\",\n",
    "    \"El corazón late al ritmo de un poema eterno\",\n",
    "    \"La lluvia besa la tierra en un dulce abrazo\",\n",
    "    \"Un arco iris florece tras la tormenta\",\n",
    "    \"El viento susurra secretos al oído del bosque\",\n",
    "    \"Un pétalo cae, marcando el paso del tiempo\",\n",
    "    \"El sol abraza el horizonte con su luz dorada\",\n",
    "    \"Las nubes pintan sueños en el cielo infinito\",\n",
    "    \"El mar guarda en su canto los misterios del mundo\",\n",
    "    \"Un árbol solitario observa el paso de las estaciones\",\n",
    "    \"La luna guía al viajero perdido en la noche\",\n",
    "    \"El fuego danza al ritmo de un canto ancestral\",\n",
    "    \"Las montañas guardan los secretos de los siglos\",\n",
    "    \"El amor florece en el rincón más inesperado\",\n",
    "    \"Una lágrima cae, reflejo de un alma perdida\",\n",
    "    \"El tiempo esculpe la vida como un artista paciente\",\n",
    "    \"El silencio de la madrugada guarda mil promesas\",\n",
    "    \"Un poema se escribe en la brisa del atardecer\",\n",
    "    \"La luz de la luna ilumina los senderos olvidados\",\n",
    "    \"El eco de los sueños resuena en el valle\",\n",
    "    \"Las estrellas caen como lágrimas del universo\",\n",
    "    \"Un amanecer promete nuevos comienzos\",\n",
    "    \"El agua del río limpia las heridas del alma\",\n",
    "    \"La naturaleza canta una sinfonía de vida y muerte\",\n",
    "    \"Un viajero solitario encuentra paz en la montaña\",\n",
    "    \"La nostalgia se esconde en el murmullo del viento\",\n",
    "    \"El cielo nocturno guarda los suspiros del pasado\",\n",
    "    \"Las olas del mar susurran cuentos de sirenas\",\n",
    "    \"El aroma del jazmín embriaga la noche\",\n",
    "    \"Un atardecer rojo marca el fin de un día eterno\",\n",
    "    \"La esperanza renace con cada amanecer\",\n",
    "    \"El rocío decora las flores como lágrimas de la mañana\",\n",
    "    \"Las sombras de los árboles bailan bajo la luna\",\n",
    "    \"El amor eterno se graba en los anillos de un árbol\",\n",
    "    \"Un sendero de flores guía al caminante perdido\",\n",
    "    \"El canto de los pájaros despide al sol\",\n",
    "    \"El crepúsculo tiñe el cielo con colores de fuego\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizador de Hugging Face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar un token de relleno si no existe\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento: convertir textos a índices de tokens\n",
    "def encode_texts(corpus, max_length=20):\n",
    "    encoded_texts = tokenizer(corpus, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    return encoded_texts[\"input_ids\"], encoded_texts[\"attention_mask\"]\n",
    "\n",
    "input_ids, attention_masks = encode_texts(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Transformer simple para generación de texto\n",
    "class PoetryTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, n_heads, num_layers, ff_dim, max_seq_len):\n",
    "        super(PoetryTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, emb_dim))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=n_heads, dim_feedforward=ff_dim, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        embedded = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        encoded = self.encoder(embedded, src_key_padding_mask=~mask)  # Invierte la máscara\n",
    "        logits = self.fc(encoded)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar el modelo\n",
    "emb_dim = 128\n",
    "n_heads = 4\n",
    "num_layers = 3\n",
    "ff_dim = 256\n",
    "max_seq_len = input_ids.size(1)\n",
    "\n",
    "model = PoetryTransformer(vocab_size, emb_dim, n_heads, num_layers, ff_dim, max_seq_len).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de entrenamiento\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.3194\n",
      "Epoch 2/100, Loss: 0.2951\n",
      "Epoch 3/100, Loss: 0.2739\n",
      "Epoch 4/100, Loss: 0.2531\n",
      "Epoch 5/100, Loss: 0.2365\n",
      "Epoch 6/100, Loss: 0.2210\n",
      "Epoch 7/100, Loss: 0.2054\n",
      "Epoch 8/100, Loss: 0.1943\n",
      "Epoch 9/100, Loss: 0.1805\n",
      "Epoch 10/100, Loss: 0.1710\n",
      "Epoch 11/100, Loss: 0.1586\n",
      "Epoch 12/100, Loss: 0.1501\n",
      "Epoch 13/100, Loss: 0.1423\n",
      "Epoch 14/100, Loss: 0.1349\n",
      "Epoch 15/100, Loss: 0.1287\n",
      "Epoch 16/100, Loss: 0.1219\n",
      "Epoch 17/100, Loss: 0.1160\n",
      "Epoch 18/100, Loss: 0.1097\n",
      "Epoch 19/100, Loss: 0.1053\n",
      "Epoch 20/100, Loss: 0.1009\n",
      "Epoch 21/100, Loss: 0.0971\n",
      "Epoch 22/100, Loss: 0.0927\n",
      "Epoch 23/100, Loss: 0.0887\n",
      "Epoch 24/100, Loss: 0.0861\n",
      "Epoch 25/100, Loss: 0.0822\n",
      "Epoch 26/100, Loss: 0.0808\n",
      "Epoch 27/100, Loss: 0.0775\n",
      "Epoch 28/100, Loss: 0.0761\n",
      "Epoch 29/100, Loss: 0.0740\n",
      "Epoch 30/100, Loss: 0.0700\n",
      "Epoch 31/100, Loss: 0.0685\n",
      "Epoch 32/100, Loss: 0.0667\n",
      "Epoch 33/100, Loss: 0.0647\n",
      "Epoch 34/100, Loss: 0.0628\n",
      "Epoch 35/100, Loss: 0.0610\n",
      "Epoch 36/100, Loss: 0.0606\n",
      "Epoch 37/100, Loss: 0.0587\n",
      "Epoch 38/100, Loss: 0.0571\n",
      "Epoch 39/100, Loss: 0.0551\n",
      "Epoch 40/100, Loss: 0.0547\n",
      "Epoch 41/100, Loss: 0.0539\n",
      "Epoch 42/100, Loss: 0.0523\n",
      "Epoch 43/100, Loss: 0.0512\n",
      "Epoch 44/100, Loss: 0.0498\n",
      "Epoch 45/100, Loss: 0.0496\n",
      "Epoch 46/100, Loss: 0.0485\n",
      "Epoch 47/100, Loss: 0.0475\n",
      "Epoch 48/100, Loss: 0.0465\n",
      "Epoch 49/100, Loss: 0.0459\n",
      "Epoch 50/100, Loss: 0.0451\n",
      "Epoch 51/100, Loss: 0.0436\n",
      "Epoch 52/100, Loss: 0.0435\n",
      "Epoch 53/100, Loss: 0.0428\n",
      "Epoch 54/100, Loss: 0.0417\n",
      "Epoch 55/100, Loss: 0.0412\n",
      "Epoch 56/100, Loss: 0.0404\n",
      "Epoch 57/100, Loss: 0.0394\n",
      "Epoch 58/100, Loss: 0.0391\n",
      "Epoch 59/100, Loss: 0.0388\n",
      "Epoch 60/100, Loss: 0.0381\n",
      "Epoch 61/100, Loss: 0.0378\n",
      "Epoch 62/100, Loss: 0.0367\n",
      "Epoch 63/100, Loss: 0.0366\n",
      "Epoch 64/100, Loss: 0.0358\n",
      "Epoch 65/100, Loss: 0.0353\n",
      "Epoch 66/100, Loss: 0.0349\n",
      "Epoch 67/100, Loss: 0.0343\n",
      "Epoch 68/100, Loss: 0.0340\n",
      "Epoch 69/100, Loss: 0.0332\n",
      "Epoch 70/100, Loss: 0.0328\n",
      "Epoch 71/100, Loss: 0.0328\n",
      "Epoch 72/100, Loss: 0.0320\n",
      "Epoch 73/100, Loss: 0.0319\n",
      "Epoch 74/100, Loss: 0.0314\n",
      "Epoch 75/100, Loss: 0.0308\n",
      "Epoch 76/100, Loss: 0.0308\n",
      "Epoch 77/100, Loss: 0.0303\n",
      "Epoch 78/100, Loss: 0.0296\n",
      "Epoch 79/100, Loss: 0.0298\n",
      "Epoch 80/100, Loss: 0.0292\n",
      "Epoch 81/100, Loss: 0.0290\n",
      "Epoch 82/100, Loss: 0.0285\n",
      "Epoch 83/100, Loss: 0.0284\n",
      "Epoch 84/100, Loss: 0.0279\n",
      "Epoch 85/100, Loss: 0.0275\n",
      "Epoch 86/100, Loss: 0.0269\n",
      "Epoch 87/100, Loss: 0.0269\n",
      "Epoch 88/100, Loss: 0.0263\n",
      "Epoch 89/100, Loss: 0.0261\n",
      "Epoch 90/100, Loss: 0.0260\n",
      "Epoch 91/100, Loss: 0.0257\n",
      "Epoch 92/100, Loss: 0.0257\n",
      "Epoch 93/100, Loss: 0.0250\n",
      "Epoch 94/100, Loss: 0.0249\n",
      "Epoch 95/100, Loss: 0.0247\n",
      "Epoch 96/100, Loss: 0.0244\n",
      "Epoch 97/100, Loss: 0.0239\n",
      "Epoch 98/100, Loss: 0.0240\n",
      "Epoch 99/100, Loss: 0.0236\n",
      "Epoch 100/100, Loss: 0.0234\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 100\n",
    "input_ids, attention_masks = input_ids.to(device), attention_masks.bool().to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(input_ids, mask=attention_masks)\n",
    "    loss = criterion(outputs.view(-1, vocab_size), input_ids.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de generación de texto\n",
    "def generate_poetry(prompt, model, max_length=20):\n",
    "    model.eval()\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=max_length)[\"input_ids\"].to(device)\n",
    "    \n",
    "    generated = prompt_ids\n",
    "    for _ in range(max_length - len(prompt_ids[0])):\n",
    "        outputs = model(generated)\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        generated = torch.cat((generated, next_token), dim=1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generación de poesía:\n",
      "\n",
      "en el mar\n"
     ]
    }
   ],
   "source": [
    "# Generar poesía con un prompt\n",
    "prompt = \"En el mar\"\n",
    "poem = generate_poetry(prompt, model, 20)\n",
    "print(\"\\nGeneración de poesía:\\n\")\n",
    "print(poem)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
